{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94ff406-ed49-4c3b-8471-51deabe80391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#汽车处理图像转化为模型输入\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def slice_image(image):\n",
    "    \"\"\"\n",
    "    提取图像的下半部分。\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像。\n",
    "    \n",
    "    返回：\n",
    "    - 图像的下半部分。\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    # 计算下半部分的裁剪区域\n",
    "    box = (0, height // 2, width, height)\n",
    "    lower_half = image.crop(box)\n",
    "    \n",
    "    return lower_half\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    处理单张图片：提取下半部分并应用Canny边缘检测。\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像。\n",
    "    \n",
    "    返回：\n",
    "    - 处理后的图像（Canny边缘检测结果），大小为 (32, 32)。\n",
    "    \"\"\"\n",
    "    # 第一步：提取图片的下半部分\n",
    "    lower_half = slice_image(image)\n",
    "    \n",
    "    # 第二步：将下半部分转换为OpenCV格式\n",
    "    lower_half_cv = cv2.cvtColor(np.array(lower_half), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 第三步：转换为灰度图\n",
    "    gray = cv2.cvtColor(lower_half_cv, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 第四步：应用高斯模糊，减少噪声\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # 第五步：应用Canny边缘检测\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # 确保输出图像大小为 (32, 32)\n",
    "    edges_resized = cv2.resize(edges, (64, 64))  # 将图像调整为32x32大小\n",
    "    \n",
    "    return edges_resized\n",
    "\n",
    "def process_image_for_model(image):\n",
    "    \"\"\"\n",
    "    处理图像以便输入到模型：裁剪下半部分，调整大小，转换为张量并进行归一化。\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像，假设是 NumPy 数组格式。\n",
    "    \n",
    "    返回：\n",
    "    - 预处理后的图像张量\n",
    "    \"\"\"\n",
    "    # 将 NumPy 数组转换为 PIL 图像\n",
    "    result_image_pil = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "    # 确保图像是 RGB 格式\n",
    "    if result_image_pil.mode != 'RGB':\n",
    "        result_image_pil = result_image_pil.convert('RGB')\n",
    "    \n",
    "    # 定义 transform（已经给定了）\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # 确保图像大小为32x32\n",
    "        transforms.ToTensor(),        # 将图像转换为 Tensor 格式\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化\n",
    "    ])\n",
    "    \n",
    "    # 对图像进行预处理\n",
    "    image_tensor = transform(result_image_pil).unsqueeze(0)  # 增加 batch 维度\n",
    "    return image_tensor\n",
    "\n",
    "def process_and_prepare_for_model(image):\n",
    "    \"\"\"\n",
    "    整合图像处理与预处理流程：\n",
    "    1. 提取图像下半部分并进行Canny边缘检测\n",
    "    2. 将处理后的图像转换为模型可用的格式\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像，假设为 PIL 图像\n",
    "    \n",
    "    返回：\n",
    "    - 处理并准备好的图像张量\n",
    "    \"\"\"\n",
    "    # 步骤1：处理图像（提取下半部分并进行边缘检测）\n",
    "    processed_image = process_image(slice_image(image))\n",
    "    \n",
    "    # 步骤2：将处理后的图像转换为模型输入格式\n",
    "    # 由于 process_image 返回的是 NumPy 数组，先转换为模型需要的格式\n",
    "    return process_image_for_model(processed_image)\n",
    "\n",
    "# 定义模型结构（在model set中）\n",
    "def convnet1(image_size=64):\n",
    "    return nn.Sequential(\n",
    "        # 第一层卷积层，减少通道数\n",
    "        nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1),  # 输出：64x64x4\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # 输出：32x32x4\n",
    "\n",
    "        # 第二层卷积层\n",
    "        nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1),  # 输出：32x32x8\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # 输出：16x16x8\n",
    "\n",
    "        # 第三层卷积层\n",
    "        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # 输出：16x16x16\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # 输出：8x8x16\n",
    "        \n",
    "        nn.Flatten(),  # Flatten the output\n",
    "\n",
    "        # 全连接层\n",
    "        nn.Linear(16 * 8 * 8, 128),  # 修改全连接层输入大小：16 * 8 * 8 = 1024\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # Dropout 层，减少过拟合\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        # 输出层\n",
    "        nn.Linear(128, 3)  # 输出 3 个类别\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed0285d-d288-40b9-9b18-37ab74c5cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'C:\\Users\\yil\\Desktop\\ipy project\\bot-project\\transfer_picture\\Driving-picture\\2-angle+.jpg'\n",
    "\n",
    "# 打开并加载图片\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fd23a4-53c0-46cc-9a5b-9cdce08a9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    }
   ],
   "source": [
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1534d1d4-9d37-4c6f-b110-b0c3a8cc4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理并准备图像以便输入到模型\n",
    "model_input = process_and_prepare_for_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "486765b3-d792-4fcf-8f33-b9b178d93cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yil\\AppData\\Local\\Temp\\ipykernel_1900\\469306162.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load('model_weights_3.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新构建相同的模型结构\n",
    "model = convnet1(image_size=64)\n",
    "\n",
    "# 加载保存的权重\n",
    "model_weights = torch.load('model_weights_3.pth')\n",
    "model.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0641b109-b739-44fb-ba87-ecbeca7531a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Flatten(start_dim=1, end_dim=-1)\n",
       "  (10): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): Dropout(p=0.3, inplace=False)\n",
       "  (13): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8fa1578-d7d7-4c05-82c4-c393466e7ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(model_input)\n",
    "\n",
    "# 获取预测类别\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1265a651-771a-4d23-aaba-827d03a06b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(predicted_class.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1cc5a-03d8-47fa-90df-293e5869c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing for car images to model input\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def slice_image(image):\n",
    "    \"\"\"\n",
    "    Extract the lower half of the image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image.\n",
    "    \n",
    "    Returns:\n",
    "    - The lower half of the image.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    # Calculate the cropping area for the lower half\n",
    "    box = (0, height // 2, width, height)\n",
    "    lower_half = image.crop(box)\n",
    "    \n",
    "    return lower_half\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    Process a single image: extract the lower half and apply Canny edge detection.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image.\n",
    "    \n",
    "    Returns:\n",
    "    - The processed image (Canny edge detection result), resized to (32, 32).\n",
    "    \"\"\"\n",
    "    # Step 1: Extract the lower half of the image\n",
    "    lower_half = slice_image(image)\n",
    "    \n",
    "    # Step 2: Convert the lower half to OpenCV format\n",
    "    lower_half_cv = cv2.cvtColor(np.array(lower_half), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Step 3: Convert to grayscale\n",
    "    gray = cv2.cvtColor(lower_half_cv, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Step 4: Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Step 5: Apply Canny edge detection\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # Ensure the output image is resized to (32, 32)\n",
    "    edges_resized = cv2.resize(edges, (64, 64))  # Resize to 64x64\n",
    "    \n",
    "    return edges_resized\n",
    "\n",
    "def process_image_for_model(image):\n",
    "    \"\"\"\n",
    "    Process the image for model input: extract the lower half, resize, convert to tensor and normalize.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image, assumed to be in NumPy array format.\n",
    "    \n",
    "    Returns:\n",
    "    - The preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    # Convert the NumPy array to a PIL image\n",
    "    result_image_pil = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "    # Ensure the image is in RGB format\n",
    "    if result_image_pil.mode != 'RGB':\n",
    "        result_image_pil = result_image_pil.convert('RGB')\n",
    "    \n",
    "    # Define the transformations (already provided)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # Ensure the image size is 64x64\n",
    "        transforms.ToTensor(),        # Convert the image to Tensor format\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformations\n",
    "    image_tensor = transform(result_image_pil).unsqueeze(0)  # Add the batch dimension\n",
    "    return image_tensor\n",
    "\n",
    "def process_and_prepare_for_model(image):\n",
    "    \"\"\"\n",
    "    Integrate the image processing and preprocessing steps:\n",
    "    1. Extract the lower half of the image and apply Canny edge detection.\n",
    "    2. Convert the processed image into a format suitable for the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image, assumed to be a PIL image.\n",
    "    \n",
    "    Returns:\n",
    "    - The processed and prepared image tensor for the model.\n",
    "    \"\"\"\n",
    "    # Step 1: Process the image (extract the lower half and apply edge detection)\n",
    "    processed_image = process_image(slice_image(image))\n",
    "    \n",
    "    # Step 2: Convert the processed image into the model's input format\n",
    "    # Since process_image returns a NumPy array, it is first converted to the required format\n",
    "    return process_image_for_model(processed_image)\n",
    "\n",
    "# Define the model architecture (in the model set)\n",
    "def convnet1(image_size=64):\n",
    "    return nn.Sequential(\n",
    "        # First convolutional layer, reducing the number of channels\n",
    "        nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1),  # Output: 64x64x4\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 32x32x4\n",
    "\n",
    "        # Second convolutional layer\n",
    "        nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1),  # Output: 32x32x8\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x16x8\n",
    "\n",
    "        # Third convolutional layer\n",
    "        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # Output: 16x16x16\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 8x8x16\n",
    "        \n",
    "        nn.Flatten(),  # Flatten the output\n",
    "\n",
    "        # Fully connected layer\n",
    "        nn.Linear(16 * 8 * 8, 128),  # Adjusted fully connected layer input size: 16 * 8 * 8 = 1024\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # Dropout layer to reduce overfitting\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        # Output layer\n",
    "        nn.Linear(128, 3)  # Output 3 categories\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opencv_env)",
   "language": "python",
   "name": "opencv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
