{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df895ed-448c-469f-a411-b131cf6a8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#汽车处理图像转化为模型输入\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0759fa22-c2b1-4af3-95c3-cc9d0647bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#汽车处理图像转化为模型输入\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    处理单张图片：提取下半部分并应用Canny边缘检测。\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像。\n",
    "    \n",
    "    返回：\n",
    "    - 处理后的图像（Canny边缘检测结果），大小为 (32, 32)。\n",
    "    \"\"\"\n",
    "    # 第一步：提取图片的下半部分\n",
    "    lower_half = slice_image(image)\n",
    "    \n",
    "    # 第二步：将下半部分转换为OpenCV格式\n",
    "    lower_half_cv = cv2.cvtColor(np.array(lower_half), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 第三步：转换为灰度图\n",
    "    gray = cv2.cvtColor(lower_half_cv, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 第四步：应用高斯模糊，减少噪声\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # 第五步：应用Canny边缘检测\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # 确保输出图像大小为 (32, 32)\n",
    "    edges_resized = cv2.resize(edges, (64, 64))  # 将图像调整为32x32大小\n",
    "    \n",
    "    return edges_resized\n",
    "\n",
    "def process_image_for_model(image):\n",
    "    \"\"\"\n",
    "    处理图像以便输入到模型：裁剪下半部分，调整大小，转换为张量并进行归一化。\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像，假设是 NumPy 数组格式。\n",
    "    \n",
    "    返回：\n",
    "    - 预处理后的图像张量\n",
    "    \"\"\"\n",
    "    # 将 NumPy 数组转换为 PIL 图像\n",
    "    result_image_pil = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "    # 确保图像是 RGB 格式\n",
    "    if result_image_pil.mode != 'RGB':\n",
    "        result_image_pil = result_image_pil.convert('RGB')\n",
    "    \n",
    "    # 定义 transform（已经给定了）\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # 确保图像大小为32x32\n",
    "        transforms.ToTensor(),        # 将图像转换为 Tensor 格式\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化\n",
    "    ])\n",
    "    \n",
    "    # 对图像进行预处理\n",
    "    image_tensor = transform(result_image_pil).unsqueeze(0)  # 增加 batch 维度\n",
    "    return image_tensor\n",
    "\n",
    "def process_and_prepare_for_model(image):\n",
    "    \"\"\"\n",
    "    整合图像处理与预处理流程：\n",
    "    1. 提取图像下半部分并进行Canny边缘检测\n",
    "    2. 将处理后的图像转换为模型可用的格式\n",
    "    \n",
    "    参数：\n",
    "    - image: 输入的图像，假设为 PIL 图像\n",
    "    \n",
    "    返回：\n",
    "    - 处理并准备好的图像张量\n",
    "    \"\"\"\n",
    "    # 步骤1：处理图像（提取下半部分并进行边缘检测）\n",
    "    processed_image = process_image(slice_image(image))\n",
    "    \n",
    "    # 步骤2：将处理后的图像转换为模型输入格式\n",
    "    # 由于 process_image 返回的是 NumPy 数组，先转换为模型需要的格式\n",
    "    return process_image_for_model(processed_image)\n",
    "\n",
    "# 定义模型结构（在model set中）\n",
    "def convnet1(image_size=64):\n",
    "    return nn.Sequential(\n",
    "        # 第一层卷积层，减少通道数\n",
    "        nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1),  # 输出：64x64x4\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # 输出：32x32x4\n",
    "\n",
    "        # 第二层卷积层\n",
    "        nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1),  # 输出：32x32x8\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # 输出：16x16x8\n",
    "\n",
    "        # 第三层卷积层\n",
    "        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # 输出：16x16x16\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # 输出：8x8x16\n",
    "        \n",
    "        nn.Flatten(),  # Flatten the output\n",
    "\n",
    "        # 全连接层\n",
    "        nn.Linear(16 * 8 * 8, 128),  # 修改全连接层输入大小：16 * 8 * 8 = 1024\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # Dropout 层，减少过拟合\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        # 输出层\n",
    "        nn.Linear(128, 3)  # 输出 3 个类别\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d4d0d1-6c4a-45c2-ab55-d8b5cf1ff758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yil\\AppData\\Local\\Temp\\ipykernel_26836\\3489696964.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load('model_weights_3.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Flatten(start_dim=1, end_dim=-1)\n",
       "  (10): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): Dropout(p=0.3, inplace=False)\n",
       "  (13): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新构建相同的模型结构\n",
    "model = convnet1(image_size=64)\n",
    "\n",
    "# 加载保存的权重\n",
    "model_weights = torch.load('model_weights_3.pth')\n",
    "model.load_state_dict(model_weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31655bab-b428-4407-8acb-ea087d544a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 while 循环打印 1 到 5\n",
    "i = 1\n",
    "while i <= 5:\n",
    "    i += 1\n",
    "    \n",
    "    image_path = r'C:\\Users\\yil\\Desktop\\ipy project\\bot-project\\transfer_picture\\Driving-picture\\original26.jpg'\n",
    "    # 打开并加载图片\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # 处理并准备图像以便输入到模型\n",
    "    model_input = process_and_prepare_for_model(image)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(model_input)\n",
    "\n",
    "    # 获取预测类别\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    print(f\"Predicted class: {predicted_class.item()}\")\n",
    "\n",
    "    if predicted_class.item() == 1:\n",
    "        print(1)\n",
    "    else:\n",
    "        print(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opencv_env)",
   "language": "python",
   "name": "opencv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
